
This project aims to address these challenges by developing a robust hand sign 
recognition system capable of identifying specific gestures with high accuracy. 
Leveraging Mediapipe for precise hand landmark detection, the system efficiently 
captures the intricate movements and positions of fingers and hands. A custom-trained 
machine learning model processes this data, ensuring reliable gesture classification 
across diverse environments. 

There is no available sign language available. Therefore, I have created my own. This 
required me to create the labeled dataset myself by capturing frames using cv2 library. 
 
The dataset employed for this hand sign recognition project is a carefully curated collection 
of 4,500 images, specifically designed to train and evaluate the machine learning model. It 
comprises 500 labeled samples for each of the nine distinct hand symbols, ensuring a 
balanced representation across all gestures. 

Use
git clone https://github.com/PurpleHyacinth120/Hand-Sign-Recognition
to setup the repository locally.
